{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "julia EM model fitting example, Nathaniel Daw 8/2020\n",
    "\n",
    "####### NOTE NOTE NOTE: PARALLEL COMPUTATION IS NOW AUTOMATIC IN THIS VERSION \n",
    "####### BUT TO RUN PARALLEL YOU MUST SET ENVIRONMENT VARIABLE JULIA_NUM_THREADS  \n",
    "####### BEFORE STARTING JULIA OR JUPYTER-NOTEBOOK\n",
    "\n",
    "eg in linux/bash:\n",
    "      export JULIA_NUM_THREADS=`nproc`; julia\n",
    "\n",
    "to install dependencies run:\n",
    "import Pkg\n",
    "Pkg.add([\"DataFrames\", \"ForwardDiff\", \"Optim\", \"LinearAlgebra\", \"StatsFuns\", \"SpecialFunctions\",  \"Statistics\", \"Distributions\", \"GLM\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### setup \n",
    "\n",
    "full = false    # Maintain full covariance matrix (vs a diagional one) at the group level\n",
    "emtol = 1e-3    # stopping condition (relative change) for EM\n",
    "\n",
    "# load the code\n",
    "# change this to where you keep the code\n",
    "# directory = \"/mnt/c/Users/daw/Dropbox (Princeton)/expts/julia em/git/em\"\n",
    "directory = \"/users/ndaw/Dropbox (Princeton)/expts/julia em/git/em\"\n",
    "\n",
    "push!(LOAD_PATH,directory)\n",
    "using EM\n",
    "\n",
    "# this loads additional packages used in examples below\n",
    "\n",
    "using Statistics\n",
    "using Random\n",
    "using GLM\n",
    "using DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Q learning example\n",
    "\n",
    "# simulate some  qlearning data\n",
    "\n",
    "Random.seed!(1234); # (for repeatability)\n",
    "\n",
    "NS = 250;\n",
    "NT = 200;\n",
    "NP = 2;\n",
    "\n",
    "params = zeros(NS,NP);\n",
    "\n",
    "cov = randn(NS); # simulated between-subject variable, e.g. age or IQ\n",
    "cov = cov .- mean(cov);\n",
    "\n",
    "cov2 = randn(NS); # simulated between-subject variable, e.g. age or IQ\n",
    "cov2 = cov2 .- mean(cov2);\n",
    "\n",
    "# subject level parameters\n",
    "\n",
    "params[:,1] = 1 .+ 0.5 * randn(NS) + cov; # softmax  temp: mean 1, effect of cov\n",
    "params[:,2] = 0 .+ 1 * randn(NS) + cov2;  # learning rate (pre sigmoidal transform): mean 0, effect of cov2\n",
    "\n",
    "c = zeros(Int64,NS*NT);\n",
    "r = zeros(Int64,NS*NT);\n",
    "s = zeros(Int64,NS*NT);\n",
    "\n",
    "for i = 1:NS\n",
    "\t(c[(i-1)*NT+1:i*NT],r[(i-1)*NT+1:i*NT]) = simq(params[i,:],NT);\n",
    "\ts[(i-1)*NT+1:i*NT] .= i;\n",
    "end\n",
    "\n",
    "data = DataFrame(sub=s,c=c,r=r);\n",
    "subs = 1:NS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 5.0\n",
       " 1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up the fit\n",
    "\n",
    "# design matrix specifying the group level model\n",
    "# this is replicated once for each model parameter\n",
    "#\n",
    "# in particular for each subject-level parameter x_ij  (subject i, parameter j)\n",
    "#\n",
    "# x_ij ~ Normal(X beta_j, Sigma)\n",
    "#\n",
    "# thus X has a row for each subject and a column for each predictor\n",
    "# in the simplest case where the only predictor is an intercept, X = ones(NS)\n",
    "# then beta_j specifies the group-level mean for parameter j\n",
    "#\n",
    "# but in this example we have two covariates that vary by subject\n",
    "# so x_ij = beta_1j + beta_2j * cov_i + beta_3j * cov2_i\n",
    "# and we infer the slopes beta for each parameter j as well as the intercept\n",
    "#\n",
    "# so we have a design matrix with 3 columns, and a row per subject:\n",
    "\n",
    "X = [ones(NS) cov cov2];\n",
    "\n",
    "# note: when you have no covariates (only intercepts) omit the brackets to get a column vector\n",
    "\n",
    "# X = ones(NS)\n",
    "\n",
    "# starting points for group level parameters\n",
    "# betas: one column for each parameter, one row for each regressor (so here: 3 rows, 2 columns)\n",
    "# make sure these are floats\n",
    "# note: if you have a single predictor you need a row vector (length: # params)\n",
    "# eg betas = [0. 0.];\n",
    "# and if there is also only a single model parameter and no covariates, then betas is a scalar\n",
    "# eg betas = 0.\n",
    "\n",
    "startbetas = [1. 0; 0 0; 0 0]\n",
    "\n",
    "# sigma: one element starting variance for each model parameter (this is really variance not SD)\n",
    "# if there is only one model parameter it needs to be a length-one vector eg. sigma = [5.]\n",
    "\n",
    "startsigma = [5., 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimation & significance tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iter: 9\n",
      "betas: [0.92 -0.23; 0.99 0.1; -0.04 0.76]\n",
      "sigma: [0.27, 0.74]\n",
      "free energy: -22995.277507\n",
      "change: [0.00021, -0.000331, 0.000284, 9.2e-5, -0.000974, 3.9e-5, 0.000672, 0.000147]\n",
      "max: 0.000974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3×2 Matrix{Float64}:\n",
       "  0.915928   -0.233291\n",
       "  0.985689    0.104455\n",
       " -0.0409915   0.756387"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# fit the model\n",
    "# (this takes: a data frame, a list of subjects, a group level design matrix, \n",
    "#  starting group level betas, starting group-level variance or covariance, a likelihood function\n",
    "#  and some optional options)\n",
    "#\n",
    "# (return values: betas are the group level means and slopes\n",
    "#  sigma is the group level *variance* or covariance\n",
    "#  x is a matrix of MAP/empirical Bayes per-subject parameters\n",
    "#  l is the per-subject negative log likelihoods \n",
    "#  h is the *inverse* per subject hessians) \n",
    "\n",
    "(betas,sigma,x,l,h) = em(data,subs,X,startbetas,startsigma,qlik; emtol=emtol, full=full);\n",
    "\n",
    "betas # ground truth would be [1 0 ; 1 0; 0 1] so this is closei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×2 adjoint(::Matrix{Float64}) with eltype Float64:\n",
       " 3.13697e-88  0.00011836\n",
       " 1.79943e-84  0.0712136\n",
       " 0.291688     1.26102e-26"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standard errors on the subject-level means, based on an asymptotic Gaussian approx \n",
    "# (these may be inflated for small n)\n",
    "# returns standard errors, pvalues, and a covariance matrix \n",
    "# these are a vector ordered as though the betas matrix were read out column-wise\n",
    "# eg parameter 1, (intercept covariate covariate) then parameter 2\n",
    "\n",
    "(standarderrors,pvalues,covmtx) = emerrors(data,subs,x,X,h,betas,sigma,qlik)\n",
    "reshape(pvalues,size(betas'))'  # cov1 (2nd row) is significant for beta (first column)\n",
    "# & cov2 (3rd row) is significant for alpha (second column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}}}}, Matrix{Float64}}\n",
       "\n",
       "beta ~ 1 + cov + cov2\n",
       "\n",
       "Coefficients:\n",
       "──────────────────────────────────────────────────────────────────────────\n",
       "                  Coef.  Std. Error      t  Pr(>|t|)  Lower 95%  Upper 95%\n",
       "──────────────────────────────────────────────────────────────────────────\n",
       "(Intercept)   0.892822    0.0350906  25.44    <1e-70   0.823707   0.961936\n",
       "cov           0.882962    0.0347236  25.43    <1e-70   0.814569   0.951354\n",
       "cov2         -0.0350505   0.0378879  -0.93    0.3558  -0.109675   0.039574\n",
       "──────────────────────────────────────────────────────────────────────────"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}}}}, Matrix{Float64}}\n",
       "\n",
       "alpha ~ 1 + cov + cov2\n",
       "\n",
       "Coefficients:\n",
       "───────────────────────────────────────────────────────────────────────────\n",
       "                  Coef.  Std. Error      t  Pr(>|t|)   Lower 95%  Upper 95%\n",
       "───────────────────────────────────────────────────────────────────────────\n",
       "(Intercept)  -0.200008    0.0501431  -3.99    <1e-04  -0.29877    -0.101245\n",
       "cov           0.0821765   0.0496187   1.66    0.0990  -0.0155532   0.179906\n",
       "cov2          0.622858    0.0541403  11.50    <1e-24   0.516222    0.729493\n",
       "───────────────────────────────────────────────────────────────────────────"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iter: 9\n",
      "betas: [0.89 -0.2]\n",
      "sigma: [1.17, 1.09]\n",
      "free energy: NaN\n",
      "change: [2.0e-6, -8.0e-6, 7.0e-6, 6.0e-6]\n",
      "max: 8.0e-6\n"
     ]
    }
   ],
   "source": [
    "# another way to get a p value for a covariate, by omitting it from the model and regressing\n",
    "# this seems to work better when full=false\n",
    "# in general not super well justified and can clearly be biased in some cases\n",
    "# but works well in practice as long as you avoid the bias cases (which are pretty obvious)\n",
    "\n",
    "X2 = ones(NS);\n",
    "startbetas2 = [0. 0.];\n",
    "startsigma2 = [5., 1];\n",
    "(betas2,sigma2,x2,l2,h2) = em(data,subs,X2,startbetas2,startsigma2,qlik; emtol=1e-5, full=full);\n",
    "\n",
    "display(lm(@formula(beta~cov+cov2),DataFrame(beta=x2[:,1],cov=cov,cov2=cov2)))\n",
    "display(lm(@formula(alpha~cov+cov2),DataFrame(alpha=x2[:,2],cov=cov,cov2=cov2)))\n",
    "\n",
    "# again the first covariate is significant for beta and the second for alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model comparison metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22371.741108231596"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# laplace approximation to the aggregate log marginal likelihood of the whole dataset\n",
    "# marginalized over the individual params\n",
    "\n",
    "ll1 = lml(x,l,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22415.020221369236"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to compare these between models you need to correct for the group level free parameters\n",
    "# either aic or bic (this is Quentin Huys' IBIC or IAIC, i.e. the subject level\n",
    "# params are marginalized by laplace approx, and aggregated, and the group level\n",
    "# params are corrected by AIC or BIC)\n",
    "\n",
    "ibic(x,l,h,betas,sigma,NS*NT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22379.741108231596"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iaic(x,l,h,betas,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: 1..2..3..4..5..6..7..8..9..10..11..12..13..14..15..16..17..18..19..20..21..22..23..24..25..26..27..28..29..30..31..32..33..34..35..36..37..38..39..40..41..42..43..44..45..46..47..48..49..50..51..52..53..54..55..56..57..58..59..60..61..62..63..64..65..66..67..68..69..70..71..72..73..74..75..76..77..78..79..80..81..82..83..84..85..86..87..88..89..90..91..92..93..94..95..96..97..98..99..100..101..102..103..104..105..106..107..108..109..110..111..112..113..114..115..116..117..118..119..120..121..122..123..124..125..126..127..128..129..130..131..132..133..134..135..136..137..138..139..140..141..142..143..144..145..146..147..148..149..150..151..152..153..154..155..156..157..158..159..160..161..162..163..164..165..166..167..168..169..170..171..172..173..174..175..176..177..178..179..180..181..182..183..184..185..186..187..188..189..190..191..192..193..194..195..196..197..198..199..200..201..202..203..204..205..206..207..208..209..210..211..212..213..214..215..216..217..218..219..220..221..222..223..224..225..226..227..228..229..230..231..232..233..234..235..236..237..238..239..240..241..242..243..244..245..246..247..248..249..250.."
     ]
    },
    {
     "data": {
      "text/plain": [
       "22379.640703784487"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or by computing unbiased per subject marginal likelihoods via cross validation.\n",
    "# you can do paired t tests on these between models\n",
    "# these are also appropriate for SPM_BMS etc\n",
    "\n",
    "liks = loocv(data,subs,x,X,betas,sigma,qlik;emtol=emtol, full=full)\n",
    "sum(liks)\n",
    "\n",
    "# note that iaic does an excellent job of predicting the aggregate held out likelihood\n",
    "# but importantly these are per subject scores that you can compare in paired tests\n",
    "# across models as per Stephan et al. random effects model comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
